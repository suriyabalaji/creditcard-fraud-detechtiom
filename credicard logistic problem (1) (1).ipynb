{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "green-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indonesian-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:\\\\Users\\\\admin\\\\Downloads\\\\creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "catholic-laugh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emotional-birth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-space",
   "metadata": {},
   "source": [
    "# it has no null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "responsible-excellence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "delayed-advance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "becoming-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defined-galaxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-velvet",
   "metadata": {},
   "source": [
    "# proportion of class 0 is higher than class 1 so it is a imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "postal-freeware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Class', ylabel='count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Class',hue ='Class', data=df,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cooked-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "federal-representative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-academy",
   "metadata": {},
   "source": [
    "# there are less correleation between the given variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fantastic-paste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.iloc[:,:-1].corr(), cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "classified-personality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.173963e-01</td>\n",
       "      <td>-1.059333e-02</td>\n",
       "      <td>-4.196182e-01</td>\n",
       "      <td>-1.052602e-01</td>\n",
       "      <td>1.730721e-01</td>\n",
       "      <td>-6.301647e-02</td>\n",
       "      <td>8.471437e-02</td>\n",
       "      <td>-3.694943e-02</td>\n",
       "      <td>-8.660434e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>4.473573e-02</td>\n",
       "      <td>1.440591e-01</td>\n",
       "      <td>5.114236e-02</td>\n",
       "      <td>-1.618187e-02</td>\n",
       "      <td>-2.330828e-01</td>\n",
       "      <td>-4.140710e-02</td>\n",
       "      <td>-5.134591e-03</td>\n",
       "      <td>-9.412688e-03</td>\n",
       "      <td>-0.010596</td>\n",
       "      <td>-0.012323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>0.117396</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.135835e-16</td>\n",
       "      <td>-1.227819e-15</td>\n",
       "      <td>-9.215150e-16</td>\n",
       "      <td>1.812612e-17</td>\n",
       "      <td>-6.506567e-16</td>\n",
       "      <td>-1.005191e-15</td>\n",
       "      <td>-2.433822e-16</td>\n",
       "      <td>-1.513678e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.457409e-16</td>\n",
       "      <td>-4.290944e-16</td>\n",
       "      <td>6.168652e-16</td>\n",
       "      <td>-4.425156e-17</td>\n",
       "      <td>-9.605737e-16</td>\n",
       "      <td>-1.581290e-17</td>\n",
       "      <td>1.198124e-16</td>\n",
       "      <td>2.083082e-15</td>\n",
       "      <td>-0.227709</td>\n",
       "      <td>-0.101347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>-0.010593</td>\n",
       "      <td>4.135835e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.243764e-16</td>\n",
       "      <td>-1.121065e-15</td>\n",
       "      <td>5.157519e-16</td>\n",
       "      <td>2.787346e-16</td>\n",
       "      <td>2.055934e-16</td>\n",
       "      <td>-5.377041e-17</td>\n",
       "      <td>1.978488e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.480447e-17</td>\n",
       "      <td>1.526333e-16</td>\n",
       "      <td>1.634231e-16</td>\n",
       "      <td>1.247925e-17</td>\n",
       "      <td>-4.478846e-16</td>\n",
       "      <td>2.057310e-16</td>\n",
       "      <td>-4.966953e-16</td>\n",
       "      <td>-5.093836e-16</td>\n",
       "      <td>-0.531409</td>\n",
       "      <td>0.091289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>-0.419618</td>\n",
       "      <td>-1.227819e-15</td>\n",
       "      <td>3.243764e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.711293e-16</td>\n",
       "      <td>-6.539009e-17</td>\n",
       "      <td>1.627627e-15</td>\n",
       "      <td>4.895305e-16</td>\n",
       "      <td>-1.268779e-15</td>\n",
       "      <td>5.568367e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>5.706192e-17</td>\n",
       "      <td>-1.133902e-15</td>\n",
       "      <td>-4.983035e-16</td>\n",
       "      <td>2.686834e-19</td>\n",
       "      <td>-1.104734e-15</td>\n",
       "      <td>-1.238062e-16</td>\n",
       "      <td>1.045747e-15</td>\n",
       "      <td>9.775546e-16</td>\n",
       "      <td>-0.210880</td>\n",
       "      <td>-0.192961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>-0.105260</td>\n",
       "      <td>-9.215150e-16</td>\n",
       "      <td>-1.121065e-15</td>\n",
       "      <td>4.711293e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.719944e-15</td>\n",
       "      <td>-7.491959e-16</td>\n",
       "      <td>-4.104503e-16</td>\n",
       "      <td>5.697192e-16</td>\n",
       "      <td>6.923247e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.949553e-16</td>\n",
       "      <td>-6.276051e-17</td>\n",
       "      <td>9.164206e-17</td>\n",
       "      <td>1.584638e-16</td>\n",
       "      <td>6.070716e-16</td>\n",
       "      <td>-4.247268e-16</td>\n",
       "      <td>3.977061e-17</td>\n",
       "      <td>-2.761403e-18</td>\n",
       "      <td>0.098732</td>\n",
       "      <td>0.133447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.173072</td>\n",
       "      <td>1.812612e-17</td>\n",
       "      <td>5.157519e-16</td>\n",
       "      <td>-6.539009e-17</td>\n",
       "      <td>-1.719944e-15</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.408382e-16</td>\n",
       "      <td>2.715541e-16</td>\n",
       "      <td>7.437229e-16</td>\n",
       "      <td>7.391702e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.920976e-16</td>\n",
       "      <td>1.253751e-16</td>\n",
       "      <td>-8.428683e-18</td>\n",
       "      <td>-1.149255e-15</td>\n",
       "      <td>4.808532e-16</td>\n",
       "      <td>4.319541e-16</td>\n",
       "      <td>6.590482e-16</td>\n",
       "      <td>-5.613951e-18</td>\n",
       "      <td>-0.386356</td>\n",
       "      <td>-0.094974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>-0.063016</td>\n",
       "      <td>-6.506567e-16</td>\n",
       "      <td>2.787346e-16</td>\n",
       "      <td>1.627627e-15</td>\n",
       "      <td>-7.491959e-16</td>\n",
       "      <td>2.408382e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.191668e-16</td>\n",
       "      <td>-1.104219e-16</td>\n",
       "      <td>4.131207e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>5.833316e-17</td>\n",
       "      <td>-4.705235e-19</td>\n",
       "      <td>1.046712e-16</td>\n",
       "      <td>-1.071589e-15</td>\n",
       "      <td>4.562861e-16</td>\n",
       "      <td>-1.357067e-16</td>\n",
       "      <td>-4.452461e-16</td>\n",
       "      <td>2.594754e-16</td>\n",
       "      <td>0.215981</td>\n",
       "      <td>-0.043643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.084714</td>\n",
       "      <td>-1.005191e-15</td>\n",
       "      <td>2.055934e-16</td>\n",
       "      <td>4.895305e-16</td>\n",
       "      <td>-4.104503e-16</td>\n",
       "      <td>2.715541e-16</td>\n",
       "      <td>1.191668e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.344412e-16</td>\n",
       "      <td>1.122501e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.027779e-16</td>\n",
       "      <td>-8.898922e-16</td>\n",
       "      <td>-4.387401e-16</td>\n",
       "      <td>7.434913e-18</td>\n",
       "      <td>-3.094082e-16</td>\n",
       "      <td>-9.657637e-16</td>\n",
       "      <td>-1.782106e-15</td>\n",
       "      <td>-2.776530e-16</td>\n",
       "      <td>0.397311</td>\n",
       "      <td>-0.187257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>-0.036949</td>\n",
       "      <td>-2.433822e-16</td>\n",
       "      <td>-5.377041e-17</td>\n",
       "      <td>-1.268779e-15</td>\n",
       "      <td>5.697192e-16</td>\n",
       "      <td>7.437229e-16</td>\n",
       "      <td>-1.104219e-16</td>\n",
       "      <td>3.344412e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.356078e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>3.892798e-16</td>\n",
       "      <td>2.026927e-16</td>\n",
       "      <td>6.377260e-17</td>\n",
       "      <td>-1.047097e-16</td>\n",
       "      <td>-4.653279e-16</td>\n",
       "      <td>-1.727276e-16</td>\n",
       "      <td>1.299943e-16</td>\n",
       "      <td>-6.200930e-16</td>\n",
       "      <td>-0.103079</td>\n",
       "      <td>0.019875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>-0.008660</td>\n",
       "      <td>-1.513678e-16</td>\n",
       "      <td>1.978488e-17</td>\n",
       "      <td>5.568367e-16</td>\n",
       "      <td>6.923247e-16</td>\n",
       "      <td>7.391702e-16</td>\n",
       "      <td>4.131207e-16</td>\n",
       "      <td>1.122501e-15</td>\n",
       "      <td>4.356078e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.936953e-16</td>\n",
       "      <td>-7.071869e-16</td>\n",
       "      <td>-5.214137e-16</td>\n",
       "      <td>-1.430343e-16</td>\n",
       "      <td>6.757763e-16</td>\n",
       "      <td>-7.888853e-16</td>\n",
       "      <td>-6.709655e-17</td>\n",
       "      <td>1.110541e-15</td>\n",
       "      <td>-0.044246</td>\n",
       "      <td>-0.097733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.030617</td>\n",
       "      <td>7.388135e-17</td>\n",
       "      <td>-3.991394e-16</td>\n",
       "      <td>1.156587e-15</td>\n",
       "      <td>2.232685e-16</td>\n",
       "      <td>-5.202306e-16</td>\n",
       "      <td>5.932243e-17</td>\n",
       "      <td>-7.492834e-17</td>\n",
       "      <td>-2.801370e-16</td>\n",
       "      <td>-4.642274e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.177547e-15</td>\n",
       "      <td>-6.418202e-16</td>\n",
       "      <td>3.214491e-16</td>\n",
       "      <td>-1.355885e-16</td>\n",
       "      <td>-2.846052e-16</td>\n",
       "      <td>-3.028119e-16</td>\n",
       "      <td>-2.197977e-16</td>\n",
       "      <td>4.864782e-17</td>\n",
       "      <td>-0.101502</td>\n",
       "      <td>-0.216883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>-0.247689</td>\n",
       "      <td>2.125498e-16</td>\n",
       "      <td>1.975426e-16</td>\n",
       "      <td>1.576830e-15</td>\n",
       "      <td>3.459380e-16</td>\n",
       "      <td>7.203963e-16</td>\n",
       "      <td>1.980503e-15</td>\n",
       "      <td>1.425248e-16</td>\n",
       "      <td>2.487043e-16</td>\n",
       "      <td>1.354680e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.658364e-16</td>\n",
       "      <td>7.772895e-16</td>\n",
       "      <td>-4.505332e-16</td>\n",
       "      <td>1.933267e-15</td>\n",
       "      <td>-5.600475e-16</td>\n",
       "      <td>-1.003221e-16</td>\n",
       "      <td>-2.640281e-16</td>\n",
       "      <td>-3.792314e-16</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.154876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.124348</td>\n",
       "      <td>2.053457e-16</td>\n",
       "      <td>-9.568710e-17</td>\n",
       "      <td>6.310231e-16</td>\n",
       "      <td>-5.625518e-16</td>\n",
       "      <td>7.412552e-16</td>\n",
       "      <td>2.375468e-16</td>\n",
       "      <td>-3.536655e-18</td>\n",
       "      <td>1.839891e-16</td>\n",
       "      <td>-1.079314e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>7.300527e-16</td>\n",
       "      <td>1.644699e-16</td>\n",
       "      <td>1.800885e-16</td>\n",
       "      <td>4.436512e-16</td>\n",
       "      <td>-5.712973e-16</td>\n",
       "      <td>-2.359969e-16</td>\n",
       "      <td>-4.672391e-16</td>\n",
       "      <td>6.415167e-16</td>\n",
       "      <td>-0.009542</td>\n",
       "      <td>-0.260593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>-0.065902</td>\n",
       "      <td>-2.425603e-17</td>\n",
       "      <td>6.295388e-16</td>\n",
       "      <td>2.807652e-16</td>\n",
       "      <td>1.303306e-16</td>\n",
       "      <td>5.886991e-16</td>\n",
       "      <td>-1.211182e-16</td>\n",
       "      <td>1.266462e-17</td>\n",
       "      <td>-2.921856e-16</td>\n",
       "      <td>2.251072e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.008461e-16</td>\n",
       "      <td>6.747721e-17</td>\n",
       "      <td>-7.132064e-16</td>\n",
       "      <td>-1.397470e-16</td>\n",
       "      <td>-5.497612e-16</td>\n",
       "      <td>-1.769255e-16</td>\n",
       "      <td>-4.720898e-16</td>\n",
       "      <td>1.144372e-15</td>\n",
       "      <td>0.005293</td>\n",
       "      <td>-0.004570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>-0.098757</td>\n",
       "      <td>-5.020280e-16</td>\n",
       "      <td>-1.730566e-16</td>\n",
       "      <td>4.739859e-16</td>\n",
       "      <td>2.282280e-16</td>\n",
       "      <td>6.565143e-16</td>\n",
       "      <td>2.621312e-16</td>\n",
       "      <td>2.607772e-16</td>\n",
       "      <td>-8.599156e-16</td>\n",
       "      <td>3.784757e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.356561e-16</td>\n",
       "      <td>3.740383e-16</td>\n",
       "      <td>3.883204e-16</td>\n",
       "      <td>2.003482e-16</td>\n",
       "      <td>-8.547932e-16</td>\n",
       "      <td>-1.660327e-16</td>\n",
       "      <td>1.044274e-16</td>\n",
       "      <td>2.289427e-15</td>\n",
       "      <td>0.033751</td>\n",
       "      <td>-0.302544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>-0.183453</td>\n",
       "      <td>3.547782e-16</td>\n",
       "      <td>-4.995814e-17</td>\n",
       "      <td>9.068793e-16</td>\n",
       "      <td>1.377649e-16</td>\n",
       "      <td>-8.720275e-16</td>\n",
       "      <td>-1.531188e-15</td>\n",
       "      <td>-1.690540e-16</td>\n",
       "      <td>4.127777e-16</td>\n",
       "      <td>-1.051167e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>6.605263e-17</td>\n",
       "      <td>-4.208921e-16</td>\n",
       "      <td>-3.912243e-16</td>\n",
       "      <td>-4.478263e-16</td>\n",
       "      <td>3.206423e-16</td>\n",
       "      <td>2.817791e-16</td>\n",
       "      <td>-1.143519e-15</td>\n",
       "      <td>-1.194130e-15</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.004223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>0.011903</td>\n",
       "      <td>7.212815e-17</td>\n",
       "      <td>1.177316e-17</td>\n",
       "      <td>8.299445e-16</td>\n",
       "      <td>-9.614528e-16</td>\n",
       "      <td>2.246261e-15</td>\n",
       "      <td>2.623672e-18</td>\n",
       "      <td>5.869302e-17</td>\n",
       "      <td>-5.254741e-16</td>\n",
       "      <td>-1.214086e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.715090e-16</td>\n",
       "      <td>-7.923387e-17</td>\n",
       "      <td>5.020770e-16</td>\n",
       "      <td>-3.005985e-16</td>\n",
       "      <td>-1.345418e-15</td>\n",
       "      <td>-7.290010e-16</td>\n",
       "      <td>6.789513e-16</td>\n",
       "      <td>7.588849e-16</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>-0.196539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-3.879840e-16</td>\n",
       "      <td>-2.685296e-16</td>\n",
       "      <td>7.614712e-16</td>\n",
       "      <td>-2.699612e-16</td>\n",
       "      <td>1.281914e-16</td>\n",
       "      <td>2.015618e-16</td>\n",
       "      <td>2.177192e-16</td>\n",
       "      <td>-2.269549e-16</td>\n",
       "      <td>1.113695e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.230527e-16</td>\n",
       "      <td>-8.743398e-16</td>\n",
       "      <td>3.706214e-16</td>\n",
       "      <td>-2.403828e-16</td>\n",
       "      <td>2.666806e-16</td>\n",
       "      <td>6.932833e-16</td>\n",
       "      <td>6.148525e-16</td>\n",
       "      <td>-5.534540e-17</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>-0.326481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>0.090438</td>\n",
       "      <td>3.230206e-17</td>\n",
       "      <td>3.284605e-16</td>\n",
       "      <td>1.509897e-16</td>\n",
       "      <td>-5.103644e-16</td>\n",
       "      <td>5.308590e-16</td>\n",
       "      <td>1.223814e-16</td>\n",
       "      <td>7.604126e-17</td>\n",
       "      <td>-3.667974e-16</td>\n",
       "      <td>4.993240e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.408680e-16</td>\n",
       "      <td>-4.819365e-16</td>\n",
       "      <td>-1.912006e-16</td>\n",
       "      <td>-8.986916e-17</td>\n",
       "      <td>-6.629212e-17</td>\n",
       "      <td>2.990167e-16</td>\n",
       "      <td>2.242791e-16</td>\n",
       "      <td>7.976796e-16</td>\n",
       "      <td>0.035650</td>\n",
       "      <td>-0.111485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>0.028975</td>\n",
       "      <td>1.502024e-16</td>\n",
       "      <td>-7.118719e-18</td>\n",
       "      <td>3.463522e-16</td>\n",
       "      <td>-3.980557e-16</td>\n",
       "      <td>-1.450421e-16</td>\n",
       "      <td>-1.865597e-16</td>\n",
       "      <td>-1.881008e-16</td>\n",
       "      <td>-3.875186e-16</td>\n",
       "      <td>-1.376135e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>5.115885e-16</td>\n",
       "      <td>-1.163768e-15</td>\n",
       "      <td>7.032035e-16</td>\n",
       "      <td>2.587708e-17</td>\n",
       "      <td>9.577163e-16</td>\n",
       "      <td>5.898033e-16</td>\n",
       "      <td>-2.959370e-16</td>\n",
       "      <td>-1.405379e-15</td>\n",
       "      <td>-0.056151</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>-0.050866</td>\n",
       "      <td>4.654551e-16</td>\n",
       "      <td>2.506675e-16</td>\n",
       "      <td>-9.316409e-16</td>\n",
       "      <td>-1.857247e-16</td>\n",
       "      <td>-3.554057e-16</td>\n",
       "      <td>-1.858755e-16</td>\n",
       "      <td>9.379684e-16</td>\n",
       "      <td>2.033737e-16</td>\n",
       "      <td>-2.343720e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.614597e-16</td>\n",
       "      <td>1.009285e-15</td>\n",
       "      <td>2.712885e-16</td>\n",
       "      <td>1.277215e-16</td>\n",
       "      <td>1.410054e-16</td>\n",
       "      <td>-2.803504e-16</td>\n",
       "      <td>-1.138829e-15</td>\n",
       "      <td>-2.436795e-16</td>\n",
       "      <td>0.339403</td>\n",
       "      <td>0.020090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>0.044736</td>\n",
       "      <td>-2.457409e-16</td>\n",
       "      <td>-8.480447e-17</td>\n",
       "      <td>5.706192e-17</td>\n",
       "      <td>-1.949553e-16</td>\n",
       "      <td>-3.920976e-16</td>\n",
       "      <td>5.833316e-17</td>\n",
       "      <td>-2.027779e-16</td>\n",
       "      <td>3.892798e-16</td>\n",
       "      <td>1.936953e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.649908e-15</td>\n",
       "      <td>8.119580e-16</td>\n",
       "      <td>1.761054e-16</td>\n",
       "      <td>-1.686082e-16</td>\n",
       "      <td>-5.557329e-16</td>\n",
       "      <td>-1.211281e-15</td>\n",
       "      <td>5.278775e-16</td>\n",
       "      <td>0.105999</td>\n",
       "      <td>0.040413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>0.144059</td>\n",
       "      <td>-4.290944e-16</td>\n",
       "      <td>1.526333e-16</td>\n",
       "      <td>-1.133902e-15</td>\n",
       "      <td>-6.276051e-17</td>\n",
       "      <td>1.253751e-16</td>\n",
       "      <td>-4.705235e-19</td>\n",
       "      <td>-8.898922e-16</td>\n",
       "      <td>2.026927e-16</td>\n",
       "      <td>-7.071869e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>3.649908e-15</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-7.303916e-17</td>\n",
       "      <td>9.970809e-17</td>\n",
       "      <td>-5.018575e-16</td>\n",
       "      <td>-2.503187e-17</td>\n",
       "      <td>8.461337e-17</td>\n",
       "      <td>-6.627203e-16</td>\n",
       "      <td>-0.064801</td>\n",
       "      <td>0.000805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>0.051142</td>\n",
       "      <td>6.168652e-16</td>\n",
       "      <td>1.634231e-16</td>\n",
       "      <td>-4.983035e-16</td>\n",
       "      <td>9.164206e-17</td>\n",
       "      <td>-8.428683e-18</td>\n",
       "      <td>1.046712e-16</td>\n",
       "      <td>-4.387401e-16</td>\n",
       "      <td>6.377260e-17</td>\n",
       "      <td>-5.214137e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>8.119580e-16</td>\n",
       "      <td>-7.303916e-17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.130519e-17</td>\n",
       "      <td>-8.232727e-17</td>\n",
       "      <td>1.114524e-15</td>\n",
       "      <td>2.839721e-16</td>\n",
       "      <td>1.481903e-15</td>\n",
       "      <td>-0.112633</td>\n",
       "      <td>-0.002685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>-0.016182</td>\n",
       "      <td>-4.425156e-17</td>\n",
       "      <td>1.247925e-17</td>\n",
       "      <td>2.686834e-19</td>\n",
       "      <td>1.584638e-16</td>\n",
       "      <td>-1.149255e-15</td>\n",
       "      <td>-1.071589e-15</td>\n",
       "      <td>7.434913e-18</td>\n",
       "      <td>-1.047097e-16</td>\n",
       "      <td>-1.430343e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.761054e-16</td>\n",
       "      <td>9.970809e-17</td>\n",
       "      <td>2.130519e-17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.015391e-15</td>\n",
       "      <td>1.343722e-16</td>\n",
       "      <td>-2.274142e-16</td>\n",
       "      <td>-2.819805e-16</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>-0.007221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>-0.233083</td>\n",
       "      <td>-9.605737e-16</td>\n",
       "      <td>-4.478846e-16</td>\n",
       "      <td>-1.104734e-15</td>\n",
       "      <td>6.070716e-16</td>\n",
       "      <td>4.808532e-16</td>\n",
       "      <td>4.562861e-16</td>\n",
       "      <td>-3.094082e-16</td>\n",
       "      <td>-4.653279e-16</td>\n",
       "      <td>6.757763e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.686082e-16</td>\n",
       "      <td>-5.018575e-16</td>\n",
       "      <td>-8.232727e-17</td>\n",
       "      <td>1.015391e-15</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.646517e-15</td>\n",
       "      <td>-6.406679e-16</td>\n",
       "      <td>-7.008939e-16</td>\n",
       "      <td>-0.047837</td>\n",
       "      <td>0.003308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>-0.041407</td>\n",
       "      <td>-1.581290e-17</td>\n",
       "      <td>2.057310e-16</td>\n",
       "      <td>-1.238062e-16</td>\n",
       "      <td>-4.247268e-16</td>\n",
       "      <td>4.319541e-16</td>\n",
       "      <td>-1.357067e-16</td>\n",
       "      <td>-9.657637e-16</td>\n",
       "      <td>-1.727276e-16</td>\n",
       "      <td>-7.888853e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.557329e-16</td>\n",
       "      <td>-2.503187e-17</td>\n",
       "      <td>1.114524e-15</td>\n",
       "      <td>1.343722e-16</td>\n",
       "      <td>2.646517e-15</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-3.667715e-16</td>\n",
       "      <td>-2.782204e-16</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>0.004455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>-0.005135</td>\n",
       "      <td>1.198124e-16</td>\n",
       "      <td>-4.966953e-16</td>\n",
       "      <td>1.045747e-15</td>\n",
       "      <td>3.977061e-17</td>\n",
       "      <td>6.590482e-16</td>\n",
       "      <td>-4.452461e-16</td>\n",
       "      <td>-1.782106e-15</td>\n",
       "      <td>1.299943e-16</td>\n",
       "      <td>-6.709655e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.211281e-15</td>\n",
       "      <td>8.461337e-17</td>\n",
       "      <td>2.839721e-16</td>\n",
       "      <td>-2.274142e-16</td>\n",
       "      <td>-6.406679e-16</td>\n",
       "      <td>-3.667715e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-3.061287e-16</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.017580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>-0.009413</td>\n",
       "      <td>2.083082e-15</td>\n",
       "      <td>-5.093836e-16</td>\n",
       "      <td>9.775546e-16</td>\n",
       "      <td>-2.761403e-18</td>\n",
       "      <td>-5.613951e-18</td>\n",
       "      <td>2.594754e-16</td>\n",
       "      <td>-2.776530e-16</td>\n",
       "      <td>-6.200930e-16</td>\n",
       "      <td>1.110541e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>5.278775e-16</td>\n",
       "      <td>-6.627203e-16</td>\n",
       "      <td>1.481903e-15</td>\n",
       "      <td>-2.819805e-16</td>\n",
       "      <td>-7.008939e-16</td>\n",
       "      <td>-2.782204e-16</td>\n",
       "      <td>-3.061287e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.009536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>-0.010596</td>\n",
       "      <td>-2.277087e-01</td>\n",
       "      <td>-5.314089e-01</td>\n",
       "      <td>-2.108805e-01</td>\n",
       "      <td>9.873167e-02</td>\n",
       "      <td>-3.863563e-01</td>\n",
       "      <td>2.159812e-01</td>\n",
       "      <td>3.973113e-01</td>\n",
       "      <td>-1.030791e-01</td>\n",
       "      <td>-4.424560e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.059989e-01</td>\n",
       "      <td>-6.480065e-02</td>\n",
       "      <td>-1.126326e-01</td>\n",
       "      <td>5.146217e-03</td>\n",
       "      <td>-4.783686e-02</td>\n",
       "      <td>-3.208037e-03</td>\n",
       "      <td>2.882546e-02</td>\n",
       "      <td>1.025822e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>-0.012323</td>\n",
       "      <td>-1.013473e-01</td>\n",
       "      <td>9.128865e-02</td>\n",
       "      <td>-1.929608e-01</td>\n",
       "      <td>1.334475e-01</td>\n",
       "      <td>-9.497430e-02</td>\n",
       "      <td>-4.364316e-02</td>\n",
       "      <td>-1.872566e-01</td>\n",
       "      <td>1.987512e-02</td>\n",
       "      <td>-9.773269e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>4.041338e-02</td>\n",
       "      <td>8.053175e-04</td>\n",
       "      <td>-2.685156e-03</td>\n",
       "      <td>-7.220907e-03</td>\n",
       "      <td>3.307706e-03</td>\n",
       "      <td>4.455398e-03</td>\n",
       "      <td>1.757973e-02</td>\n",
       "      <td>9.536041e-03</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time            V1            V2            V3            V4  \\\n",
       "Time    1.000000  1.173963e-01 -1.059333e-02 -4.196182e-01 -1.052602e-01   \n",
       "V1      0.117396  1.000000e+00  4.135835e-16 -1.227819e-15 -9.215150e-16   \n",
       "V2     -0.010593  4.135835e-16  1.000000e+00  3.243764e-16 -1.121065e-15   \n",
       "V3     -0.419618 -1.227819e-15  3.243764e-16  1.000000e+00  4.711293e-16   \n",
       "V4     -0.105260 -9.215150e-16 -1.121065e-15  4.711293e-16  1.000000e+00   \n",
       "V5      0.173072  1.812612e-17  5.157519e-16 -6.539009e-17 -1.719944e-15   \n",
       "V6     -0.063016 -6.506567e-16  2.787346e-16  1.627627e-15 -7.491959e-16   \n",
       "V7      0.084714 -1.005191e-15  2.055934e-16  4.895305e-16 -4.104503e-16   \n",
       "V8     -0.036949 -2.433822e-16 -5.377041e-17 -1.268779e-15  5.697192e-16   \n",
       "V9     -0.008660 -1.513678e-16  1.978488e-17  5.568367e-16  6.923247e-16   \n",
       "V10     0.030617  7.388135e-17 -3.991394e-16  1.156587e-15  2.232685e-16   \n",
       "V11    -0.247689  2.125498e-16  1.975426e-16  1.576830e-15  3.459380e-16   \n",
       "V12     0.124348  2.053457e-16 -9.568710e-17  6.310231e-16 -5.625518e-16   \n",
       "V13    -0.065902 -2.425603e-17  6.295388e-16  2.807652e-16  1.303306e-16   \n",
       "V14    -0.098757 -5.020280e-16 -1.730566e-16  4.739859e-16  2.282280e-16   \n",
       "V15    -0.183453  3.547782e-16 -4.995814e-17  9.068793e-16  1.377649e-16   \n",
       "V16     0.011903  7.212815e-17  1.177316e-17  8.299445e-16 -9.614528e-16   \n",
       "V17    -0.073297 -3.879840e-16 -2.685296e-16  7.614712e-16 -2.699612e-16   \n",
       "V18     0.090438  3.230206e-17  3.284605e-16  1.509897e-16 -5.103644e-16   \n",
       "V19     0.028975  1.502024e-16 -7.118719e-18  3.463522e-16 -3.980557e-16   \n",
       "V20    -0.050866  4.654551e-16  2.506675e-16 -9.316409e-16 -1.857247e-16   \n",
       "V21     0.044736 -2.457409e-16 -8.480447e-17  5.706192e-17 -1.949553e-16   \n",
       "V22     0.144059 -4.290944e-16  1.526333e-16 -1.133902e-15 -6.276051e-17   \n",
       "V23     0.051142  6.168652e-16  1.634231e-16 -4.983035e-16  9.164206e-17   \n",
       "V24    -0.016182 -4.425156e-17  1.247925e-17  2.686834e-19  1.584638e-16   \n",
       "V25    -0.233083 -9.605737e-16 -4.478846e-16 -1.104734e-15  6.070716e-16   \n",
       "V26    -0.041407 -1.581290e-17  2.057310e-16 -1.238062e-16 -4.247268e-16   \n",
       "V27    -0.005135  1.198124e-16 -4.966953e-16  1.045747e-15  3.977061e-17   \n",
       "V28    -0.009413  2.083082e-15 -5.093836e-16  9.775546e-16 -2.761403e-18   \n",
       "Amount -0.010596 -2.277087e-01 -5.314089e-01 -2.108805e-01  9.873167e-02   \n",
       "Class  -0.012323 -1.013473e-01  9.128865e-02 -1.929608e-01  1.334475e-01   \n",
       "\n",
       "                  V5            V6            V7            V8            V9  \\\n",
       "Time    1.730721e-01 -6.301647e-02  8.471437e-02 -3.694943e-02 -8.660434e-03   \n",
       "V1      1.812612e-17 -6.506567e-16 -1.005191e-15 -2.433822e-16 -1.513678e-16   \n",
       "V2      5.157519e-16  2.787346e-16  2.055934e-16 -5.377041e-17  1.978488e-17   \n",
       "V3     -6.539009e-17  1.627627e-15  4.895305e-16 -1.268779e-15  5.568367e-16   \n",
       "V4     -1.719944e-15 -7.491959e-16 -4.104503e-16  5.697192e-16  6.923247e-16   \n",
       "V5      1.000000e+00  2.408382e-16  2.715541e-16  7.437229e-16  7.391702e-16   \n",
       "V6      2.408382e-16  1.000000e+00  1.191668e-16 -1.104219e-16  4.131207e-16   \n",
       "V7      2.715541e-16  1.191668e-16  1.000000e+00  3.344412e-16  1.122501e-15   \n",
       "V8      7.437229e-16 -1.104219e-16  3.344412e-16  1.000000e+00  4.356078e-16   \n",
       "V9      7.391702e-16  4.131207e-16  1.122501e-15  4.356078e-16  1.000000e+00   \n",
       "V10    -5.202306e-16  5.932243e-17 -7.492834e-17 -2.801370e-16 -4.642274e-16   \n",
       "V11     7.203963e-16  1.980503e-15  1.425248e-16  2.487043e-16  1.354680e-16   \n",
       "V12     7.412552e-16  2.375468e-16 -3.536655e-18  1.839891e-16 -1.079314e-15   \n",
       "V13     5.886991e-16 -1.211182e-16  1.266462e-17 -2.921856e-16  2.251072e-15   \n",
       "V14     6.565143e-16  2.621312e-16  2.607772e-16 -8.599156e-16  3.784757e-15   \n",
       "V15    -8.720275e-16 -1.531188e-15 -1.690540e-16  4.127777e-16 -1.051167e-15   \n",
       "V16     2.246261e-15  2.623672e-18  5.869302e-17 -5.254741e-16 -1.214086e-15   \n",
       "V17     1.281914e-16  2.015618e-16  2.177192e-16 -2.269549e-16  1.113695e-15   \n",
       "V18     5.308590e-16  1.223814e-16  7.604126e-17 -3.667974e-16  4.993240e-16   \n",
       "V19    -1.450421e-16 -1.865597e-16 -1.881008e-16 -3.875186e-16 -1.376135e-16   \n",
       "V20    -3.554057e-16 -1.858755e-16  9.379684e-16  2.033737e-16 -2.343720e-16   \n",
       "V21    -3.920976e-16  5.833316e-17 -2.027779e-16  3.892798e-16  1.936953e-16   \n",
       "V22     1.253751e-16 -4.705235e-19 -8.898922e-16  2.026927e-16 -7.071869e-16   \n",
       "V23    -8.428683e-18  1.046712e-16 -4.387401e-16  6.377260e-17 -5.214137e-16   \n",
       "V24    -1.149255e-15 -1.071589e-15  7.434913e-18 -1.047097e-16 -1.430343e-16   \n",
       "V25     4.808532e-16  4.562861e-16 -3.094082e-16 -4.653279e-16  6.757763e-16   \n",
       "V26     4.319541e-16 -1.357067e-16 -9.657637e-16 -1.727276e-16 -7.888853e-16   \n",
       "V27     6.590482e-16 -4.452461e-16 -1.782106e-15  1.299943e-16 -6.709655e-17   \n",
       "V28    -5.613951e-18  2.594754e-16 -2.776530e-16 -6.200930e-16  1.110541e-15   \n",
       "Amount -3.863563e-01  2.159812e-01  3.973113e-01 -1.030791e-01 -4.424560e-02   \n",
       "Class  -9.497430e-02 -4.364316e-02 -1.872566e-01  1.987512e-02 -9.773269e-02   \n",
       "\n",
       "        ...           V21           V22           V23           V24  \\\n",
       "Time    ...  4.473573e-02  1.440591e-01  5.114236e-02 -1.618187e-02   \n",
       "V1      ... -2.457409e-16 -4.290944e-16  6.168652e-16 -4.425156e-17   \n",
       "V2      ... -8.480447e-17  1.526333e-16  1.634231e-16  1.247925e-17   \n",
       "V3      ...  5.706192e-17 -1.133902e-15 -4.983035e-16  2.686834e-19   \n",
       "V4      ... -1.949553e-16 -6.276051e-17  9.164206e-17  1.584638e-16   \n",
       "V5      ... -3.920976e-16  1.253751e-16 -8.428683e-18 -1.149255e-15   \n",
       "V6      ...  5.833316e-17 -4.705235e-19  1.046712e-16 -1.071589e-15   \n",
       "V7      ... -2.027779e-16 -8.898922e-16 -4.387401e-16  7.434913e-18   \n",
       "V8      ...  3.892798e-16  2.026927e-16  6.377260e-17 -1.047097e-16   \n",
       "V9      ...  1.936953e-16 -7.071869e-16 -5.214137e-16 -1.430343e-16   \n",
       "V10     ...  1.177547e-15 -6.418202e-16  3.214491e-16 -1.355885e-16   \n",
       "V11     ... -5.658364e-16  7.772895e-16 -4.505332e-16  1.933267e-15   \n",
       "V12     ...  7.300527e-16  1.644699e-16  1.800885e-16  4.436512e-16   \n",
       "V13     ...  1.008461e-16  6.747721e-17 -7.132064e-16 -1.397470e-16   \n",
       "V14     ... -3.356561e-16  3.740383e-16  3.883204e-16  2.003482e-16   \n",
       "V15     ...  6.605263e-17 -4.208921e-16 -3.912243e-16 -4.478263e-16   \n",
       "V16     ... -4.715090e-16 -7.923387e-17  5.020770e-16 -3.005985e-16   \n",
       "V17     ... -8.230527e-16 -8.743398e-16  3.706214e-16 -2.403828e-16   \n",
       "V18     ... -9.408680e-16 -4.819365e-16 -1.912006e-16 -8.986916e-17   \n",
       "V19     ...  5.115885e-16 -1.163768e-15  7.032035e-16  2.587708e-17   \n",
       "V20     ... -7.614597e-16  1.009285e-15  2.712885e-16  1.277215e-16   \n",
       "V21     ...  1.000000e+00  3.649908e-15  8.119580e-16  1.761054e-16   \n",
       "V22     ...  3.649908e-15  1.000000e+00 -7.303916e-17  9.970809e-17   \n",
       "V23     ...  8.119580e-16 -7.303916e-17  1.000000e+00  2.130519e-17   \n",
       "V24     ...  1.761054e-16  9.970809e-17  2.130519e-17  1.000000e+00   \n",
       "V25     ... -1.686082e-16 -5.018575e-16 -8.232727e-17  1.015391e-15   \n",
       "V26     ... -5.557329e-16 -2.503187e-17  1.114524e-15  1.343722e-16   \n",
       "V27     ... -1.211281e-15  8.461337e-17  2.839721e-16 -2.274142e-16   \n",
       "V28     ...  5.278775e-16 -6.627203e-16  1.481903e-15 -2.819805e-16   \n",
       "Amount  ...  1.059989e-01 -6.480065e-02 -1.126326e-01  5.146217e-03   \n",
       "Class   ...  4.041338e-02  8.053175e-04 -2.685156e-03 -7.220907e-03   \n",
       "\n",
       "                 V25           V26           V27           V28    Amount  \\\n",
       "Time   -2.330828e-01 -4.140710e-02 -5.134591e-03 -9.412688e-03 -0.010596   \n",
       "V1     -9.605737e-16 -1.581290e-17  1.198124e-16  2.083082e-15 -0.227709   \n",
       "V2     -4.478846e-16  2.057310e-16 -4.966953e-16 -5.093836e-16 -0.531409   \n",
       "V3     -1.104734e-15 -1.238062e-16  1.045747e-15  9.775546e-16 -0.210880   \n",
       "V4      6.070716e-16 -4.247268e-16  3.977061e-17 -2.761403e-18  0.098732   \n",
       "V5      4.808532e-16  4.319541e-16  6.590482e-16 -5.613951e-18 -0.386356   \n",
       "V6      4.562861e-16 -1.357067e-16 -4.452461e-16  2.594754e-16  0.215981   \n",
       "V7     -3.094082e-16 -9.657637e-16 -1.782106e-15 -2.776530e-16  0.397311   \n",
       "V8     -4.653279e-16 -1.727276e-16  1.299943e-16 -6.200930e-16 -0.103079   \n",
       "V9      6.757763e-16 -7.888853e-16 -6.709655e-17  1.110541e-15 -0.044246   \n",
       "V10    -2.846052e-16 -3.028119e-16 -2.197977e-16  4.864782e-17 -0.101502   \n",
       "V11    -5.600475e-16 -1.003221e-16 -2.640281e-16 -3.792314e-16  0.000104   \n",
       "V12    -5.712973e-16 -2.359969e-16 -4.672391e-16  6.415167e-16 -0.009542   \n",
       "V13    -5.497612e-16 -1.769255e-16 -4.720898e-16  1.144372e-15  0.005293   \n",
       "V14    -8.547932e-16 -1.660327e-16  1.044274e-16  2.289427e-15  0.033751   \n",
       "V15     3.206423e-16  2.817791e-16 -1.143519e-15 -1.194130e-15 -0.002986   \n",
       "V16    -1.345418e-15 -7.290010e-16  6.789513e-16  7.588849e-16 -0.003910   \n",
       "V17     2.666806e-16  6.932833e-16  6.148525e-16 -5.534540e-17  0.007309   \n",
       "V18    -6.629212e-17  2.990167e-16  2.242791e-16  7.976796e-16  0.035650   \n",
       "V19     9.577163e-16  5.898033e-16 -2.959370e-16 -1.405379e-15 -0.056151   \n",
       "V20     1.410054e-16 -2.803504e-16 -1.138829e-15 -2.436795e-16  0.339403   \n",
       "V21    -1.686082e-16 -5.557329e-16 -1.211281e-15  5.278775e-16  0.105999   \n",
       "V22    -5.018575e-16 -2.503187e-17  8.461337e-17 -6.627203e-16 -0.064801   \n",
       "V23    -8.232727e-17  1.114524e-15  2.839721e-16  1.481903e-15 -0.112633   \n",
       "V24     1.015391e-15  1.343722e-16 -2.274142e-16 -2.819805e-16  0.005146   \n",
       "V25     1.000000e+00  2.646517e-15 -6.406679e-16 -7.008939e-16 -0.047837   \n",
       "V26     2.646517e-15  1.000000e+00 -3.667715e-16 -2.782204e-16 -0.003208   \n",
       "V27    -6.406679e-16 -3.667715e-16  1.000000e+00 -3.061287e-16  0.028825   \n",
       "V28    -7.008939e-16 -2.782204e-16 -3.061287e-16  1.000000e+00  0.010258   \n",
       "Amount -4.783686e-02 -3.208037e-03  2.882546e-02  1.025822e-02  1.000000   \n",
       "Class   3.307706e-03  4.455398e-03  1.757973e-02  9.536041e-03  0.005632   \n",
       "\n",
       "           Class  \n",
       "Time   -0.012323  \n",
       "V1     -0.101347  \n",
       "V2      0.091289  \n",
       "V3     -0.192961  \n",
       "V4      0.133447  \n",
       "V5     -0.094974  \n",
       "V6     -0.043643  \n",
       "V7     -0.187257  \n",
       "V8      0.019875  \n",
       "V9     -0.097733  \n",
       "V10    -0.216883  \n",
       "V11     0.154876  \n",
       "V12    -0.260593  \n",
       "V13    -0.004570  \n",
       "V14    -0.302544  \n",
       "V15    -0.004223  \n",
       "V16    -0.196539  \n",
       "V17    -0.326481  \n",
       "V18    -0.111485  \n",
       "V19     0.034783  \n",
       "V20     0.020090  \n",
       "V21     0.040413  \n",
       "V22     0.000805  \n",
       "V23    -0.002685  \n",
       "V24    -0.007221  \n",
       "V25     0.003308  \n",
       "V26     0.004455  \n",
       "V27     0.017580  \n",
       "V28     0.009536  \n",
       "Amount  0.005632  \n",
       "Class   1.000000  \n",
       "\n",
       "[31 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-robin",
   "metadata": {},
   "source": [
    "# spliting the independent variables as x and response variable a y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "southeast-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hazardous-pickup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V20       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pregnant-darwin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-astrology",
   "metadata": {},
   "source": [
    "# spliting the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "educated-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "willing-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "finished-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "union-border",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "refined-porcelain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.40092829,   0.27214365,   0.0205792 ,  -1.42255451,\n",
       "          7.25729449,   0.85504046,  -0.66269347,  -0.07014168,\n",
       "         -1.49913052,  -2.37352128,  -3.66215718,   5.42976822,\n",
       "         -5.56468787,  -1.88722373, -10.67286436,  -0.74732535,\n",
       "         -3.17391477,  -1.93351546,  -0.0598203 ,  -0.14254562,\n",
       "         -0.10037633,   1.14400874,   0.27609889,  -0.18610155,\n",
       "         -0.66003048,  -0.4876675 ,   0.9619197 ,   0.1138351 ,\n",
       "          0.09457246,   0.51809551]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-cloud",
   "metadata": {},
   "source": [
    "# scalaing the data (this is done before fitting the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "blocked-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caroline-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.fit(x_train)\n",
    "scaled_train_array=sc.transform(x_train)\n",
    "scaled_test_array=sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "quality-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train=pd.DataFrame(data=scaled_train_array,columns=x_train.columns)\n",
    "scaled_test=pd.DataFrame(data=scaled_test_array,columns=x_test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sporting-falls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659296</td>\n",
       "      <td>0.947594</td>\n",
       "      <td>0.742974</td>\n",
       "      <td>0.767015</td>\n",
       "      <td>0.204891</td>\n",
       "      <td>0.576575</td>\n",
       "      <td>0.532272</td>\n",
       "      <td>0.502833</td>\n",
       "      <td>0.792176</td>\n",
       "      <td>0.463372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373951</td>\n",
       "      <td>0.560472</td>\n",
       "      <td>0.510278</td>\n",
       "      <td>0.669169</td>\n",
       "      <td>0.051986</td>\n",
       "      <td>0.527966</td>\n",
       "      <td>0.478826</td>\n",
       "      <td>0.658794</td>\n",
       "      <td>0.343297</td>\n",
       "      <td>0.002436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783132</td>\n",
       "      <td>0.993391</td>\n",
       "      <td>0.741886</td>\n",
       "      <td>0.746948</td>\n",
       "      <td>0.254769</td>\n",
       "      <td>0.560629</td>\n",
       "      <td>0.488531</td>\n",
       "      <td>0.499437</td>\n",
       "      <td>0.782550</td>\n",
       "      <td>0.476012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373579</td>\n",
       "      <td>0.556170</td>\n",
       "      <td>0.473264</td>\n",
       "      <td>0.669410</td>\n",
       "      <td>0.421998</td>\n",
       "      <td>0.570285</td>\n",
       "      <td>0.456546</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.339438</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.726295</td>\n",
       "      <td>0.948515</td>\n",
       "      <td>0.743667</td>\n",
       "      <td>0.818653</td>\n",
       "      <td>0.280458</td>\n",
       "      <td>0.555696</td>\n",
       "      <td>0.518814</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.773601</td>\n",
       "      <td>0.489094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375784</td>\n",
       "      <td>0.561727</td>\n",
       "      <td>0.559651</td>\n",
       "      <td>0.659332</td>\n",
       "      <td>0.517914</td>\n",
       "      <td>0.568267</td>\n",
       "      <td>0.320261</td>\n",
       "      <td>0.615360</td>\n",
       "      <td>0.307913</td>\n",
       "      <td>0.008654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.193990</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.735456</td>\n",
       "      <td>0.795014</td>\n",
       "      <td>0.221352</td>\n",
       "      <td>0.537774</td>\n",
       "      <td>0.477570</td>\n",
       "      <td>0.489833</td>\n",
       "      <td>0.785035</td>\n",
       "      <td>0.440234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373886</td>\n",
       "      <td>0.561241</td>\n",
       "      <td>0.501616</td>\n",
       "      <td>0.668079</td>\n",
       "      <td>0.463134</td>\n",
       "      <td>0.586428</td>\n",
       "      <td>0.355138</td>\n",
       "      <td>0.650365</td>\n",
       "      <td>0.341588</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.840039</td>\n",
       "      <td>0.919309</td>\n",
       "      <td>0.749305</td>\n",
       "      <td>0.796413</td>\n",
       "      <td>0.224416</td>\n",
       "      <td>0.563756</td>\n",
       "      <td>0.484713</td>\n",
       "      <td>0.508397</td>\n",
       "      <td>0.784340</td>\n",
       "      <td>0.460622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373781</td>\n",
       "      <td>0.557908</td>\n",
       "      <td>0.491690</td>\n",
       "      <td>0.662241</td>\n",
       "      <td>0.498299</td>\n",
       "      <td>0.616048</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.651842</td>\n",
       "      <td>0.345408</td>\n",
       "      <td>0.004576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199359</th>\n",
       "      <td>0.801304</td>\n",
       "      <td>0.923462</td>\n",
       "      <td>0.755053</td>\n",
       "      <td>0.760275</td>\n",
       "      <td>0.256526</td>\n",
       "      <td>0.554220</td>\n",
       "      <td>0.484195</td>\n",
       "      <td>0.502756</td>\n",
       "      <td>0.790897</td>\n",
       "      <td>0.445768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368016</td>\n",
       "      <td>0.561274</td>\n",
       "      <td>0.498951</td>\n",
       "      <td>0.665563</td>\n",
       "      <td>0.300172</td>\n",
       "      <td>0.555613</td>\n",
       "      <td>0.475310</td>\n",
       "      <td>0.636629</td>\n",
       "      <td>0.340809</td>\n",
       "      <td>0.002538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199360</th>\n",
       "      <td>0.353992</td>\n",
       "      <td>0.937540</td>\n",
       "      <td>0.752478</td>\n",
       "      <td>0.816455</td>\n",
       "      <td>0.259175</td>\n",
       "      <td>0.565025</td>\n",
       "      <td>0.522165</td>\n",
       "      <td>0.502924</td>\n",
       "      <td>0.789722</td>\n",
       "      <td>0.430408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.560652</td>\n",
       "      <td>0.509833</td>\n",
       "      <td>0.666778</td>\n",
       "      <td>0.271430</td>\n",
       "      <td>0.548589</td>\n",
       "      <td>0.472035</td>\n",
       "      <td>0.654173</td>\n",
       "      <td>0.344333</td>\n",
       "      <td>0.001271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199361</th>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.980950</td>\n",
       "      <td>0.730603</td>\n",
       "      <td>0.777036</td>\n",
       "      <td>0.176034</td>\n",
       "      <td>0.541548</td>\n",
       "      <td>0.485731</td>\n",
       "      <td>0.489524</td>\n",
       "      <td>0.783931</td>\n",
       "      <td>0.384088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370520</td>\n",
       "      <td>0.552471</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.666628</td>\n",
       "      <td>0.327683</td>\n",
       "      <td>0.594626</td>\n",
       "      <td>0.339964</td>\n",
       "      <td>0.649928</td>\n",
       "      <td>0.341224</td>\n",
       "      <td>0.001775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199362</th>\n",
       "      <td>0.562833</td>\n",
       "      <td>0.986756</td>\n",
       "      <td>0.721817</td>\n",
       "      <td>0.778821</td>\n",
       "      <td>0.221793</td>\n",
       "      <td>0.532185</td>\n",
       "      <td>0.497315</td>\n",
       "      <td>0.480019</td>\n",
       "      <td>0.787276</td>\n",
       "      <td>0.523254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377261</td>\n",
       "      <td>0.564412</td>\n",
       "      <td>0.526422</td>\n",
       "      <td>0.667151</td>\n",
       "      <td>0.329630</td>\n",
       "      <td>0.544440</td>\n",
       "      <td>0.373347</td>\n",
       "      <td>0.649001</td>\n",
       "      <td>0.340172</td>\n",
       "      <td>0.008715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199363</th>\n",
       "      <td>0.433394</td>\n",
       "      <td>0.938319</td>\n",
       "      <td>0.750477</td>\n",
       "      <td>0.823568</td>\n",
       "      <td>0.289991</td>\n",
       "      <td>0.556151</td>\n",
       "      <td>0.495532</td>\n",
       "      <td>0.498559</td>\n",
       "      <td>0.785758</td>\n",
       "      <td>0.452997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375695</td>\n",
       "      <td>0.561336</td>\n",
       "      <td>0.510048</td>\n",
       "      <td>0.661897</td>\n",
       "      <td>0.344626</td>\n",
       "      <td>0.564234</td>\n",
       "      <td>0.360714</td>\n",
       "      <td>0.652463</td>\n",
       "      <td>0.344557</td>\n",
       "      <td>0.000813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199364 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0       0.659296  0.947594  0.742974  0.767015  0.204891  0.576575  0.532272   \n",
       "1       0.783132  0.993391  0.741886  0.746948  0.254769  0.560629  0.488531   \n",
       "2       0.726295  0.948515  0.743667  0.818653  0.280458  0.555696  0.518814   \n",
       "3       0.193990  0.977700  0.735456  0.795014  0.221352  0.537774  0.477570   \n",
       "4       0.840039  0.919309  0.749305  0.796413  0.224416  0.563756  0.484713   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "199359  0.801304  0.923462  0.755053  0.760275  0.256526  0.554220  0.484195   \n",
       "199360  0.353992  0.937540  0.752478  0.816455  0.259175  0.565025  0.522165   \n",
       "199361  0.443400  0.980950  0.730603  0.777036  0.176034  0.541548  0.485731   \n",
       "199362  0.562833  0.986756  0.721817  0.778821  0.221793  0.532185  0.497315   \n",
       "199363  0.433394  0.938319  0.750477  0.823568  0.289991  0.556151  0.495532   \n",
       "\n",
       "              V7        V8        V9  ...       V20       V21       V22  \\\n",
       "0       0.502833  0.792176  0.463372  ...  0.373951  0.560472  0.510278   \n",
       "1       0.499437  0.782550  0.476012  ...  0.373579  0.556170  0.473264   \n",
       "2       0.515140  0.773601  0.489094  ...  0.375784  0.561727  0.559651   \n",
       "3       0.489833  0.785035  0.440234  ...  0.373886  0.561241  0.501616   \n",
       "4       0.508397  0.784340  0.460622  ...  0.373781  0.557908  0.491690   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "199359  0.502756  0.790897  0.445768  ...  0.368016  0.561274  0.498951   \n",
       "199360  0.502924  0.789722  0.430408  ...  0.376344  0.560652  0.509833   \n",
       "199361  0.489524  0.783931  0.384088  ...  0.370520  0.552471  0.445783   \n",
       "199362  0.480019  0.787276  0.523254  ...  0.377261  0.564412  0.526422   \n",
       "199363  0.498559  0.785758  0.452997  ...  0.375695  0.561336  0.510048   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28    Amount  \n",
       "0       0.669169  0.051986  0.527966  0.478826  0.658794  0.343297  0.002436  \n",
       "1       0.669410  0.421998  0.570285  0.456546  0.648003  0.339438  0.000101  \n",
       "2       0.659332  0.517914  0.568267  0.320261  0.615360  0.307913  0.008654  \n",
       "3       0.668079  0.463134  0.586428  0.355138  0.650365  0.341588  0.000303  \n",
       "4       0.662241  0.498299  0.616048  0.509000  0.651842  0.345408  0.004576  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "199359  0.665563  0.300172  0.555613  0.475310  0.636629  0.340809  0.002538  \n",
       "199360  0.666778  0.271430  0.548589  0.472035  0.654173  0.344333  0.001271  \n",
       "199361  0.666628  0.327683  0.594626  0.339964  0.649928  0.341224  0.001775  \n",
       "199362  0.667151  0.329630  0.544440  0.373347  0.649001  0.340172  0.008715  \n",
       "199363  0.661897  0.344626  0.564234  0.360714  0.652463  0.344557  0.000813  \n",
       "\n",
       "[199364 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "annual-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = lr.predict(scaled_train)\n",
    "pred_test = lr.predict(scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "arbitrary-approach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "social-hartford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "paperback-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-receiver",
   "metadata": {},
   "source": [
    "# classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stuck-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199019\n",
      "           1       0.87      0.54      0.66       345\n",
      "\n",
      "    accuracy                           1.00    199364\n",
      "   macro avg       0.93      0.77      0.83    199364\n",
      "weighted avg       1.00      1.00      1.00    199364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "comparable-columbus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.86      0.52      0.65       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.93      0.76      0.82     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-chocolate",
   "metadata": {},
   "source": [
    "# kappa score above 0.5 is good model so we built a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fatty-lyric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(cohen_kappa_score(y_train, pred_train),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "whole-shelter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[198990,     29],\n",
       "       [   159,    186]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "welcome-workstation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85284,    12],\n",
       "       [   71,    76]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "unnecessary-worker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.663839030619086"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_train,pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "third-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6463528349767841"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "surprising-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fossil-swedish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003921\n",
      "         Iterations 15\n"
     ]
    }
   ],
   "source": [
    "logimodel = Logit(y_train,scaled_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "empty-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>199364</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>199334</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    29</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 29 Apr 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.6921</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>01:15:47</td>     <th>  Log-Likelihood:    </th> <td> -781.75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -2538.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th>   <td>   -0.6590</td> <td>    0.467</td> <td>   -1.411</td> <td> 0.158</td> <td>   -1.574</td> <td>    0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>     <td>    2.2145</td> <td>    2.727</td> <td>    0.812</td> <td> 0.417</td> <td>   -3.130</td> <td>    7.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>     <td>   17.3211</td> <td>    5.955</td> <td>    2.909</td> <td> 0.004</td> <td>    5.649</td> <td>   28.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>     <td>    1.1953</td> <td>    2.981</td> <td>    0.401</td> <td> 0.688</td> <td>   -4.648</td> <td>    7.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>     <td>   17.9131</td> <td>    2.602</td> <td>    6.884</td> <td> 0.000</td> <td>   12.813</td> <td>   23.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>     <td>   21.5616</td> <td>    5.615</td> <td>    3.840</td> <td> 0.000</td> <td>   10.557</td> <td>   32.567</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>     <td>   -4.5520</td> <td>    4.258</td> <td>   -1.069</td> <td> 0.285</td> <td>  -12.897</td> <td>    3.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>     <td>    1.1457</td> <td>    9.899</td> <td>    0.116</td> <td> 0.908</td> <td>  -18.257</td> <td>   20.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>     <td>   -7.6560</td> <td>    5.709</td> <td>   -1.341</td> <td> 0.180</td> <td>  -18.845</td> <td>    3.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>     <td>    2.3513</td> <td>    3.767</td> <td>    0.624</td> <td> 0.533</td> <td>   -5.032</td> <td>    9.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>    <td>  -43.4730</td> <td>    8.089</td> <td>   -5.375</td> <td> 0.000</td> <td>  -59.326</td> <td>  -27.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>    <td>    1.5285</td> <td>    1.615</td> <td>    0.946</td> <td> 0.344</td> <td>   -1.637</td> <td>    4.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>    <td>    6.4494</td> <td>    2.839</td> <td>    2.271</td> <td> 0.023</td> <td>    0.884</td> <td>   12.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>    <td>   -4.0916</td> <td>    1.071</td> <td>   -3.820</td> <td> 0.000</td> <td>   -6.191</td> <td>   -1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>    <td>  -17.6107</td> <td>    2.463</td> <td>   -7.151</td> <td> 0.000</td> <td>  -22.437</td> <td>  -12.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>    <td>   -0.9541</td> <td>    1.062</td> <td>   -0.898</td> <td> 0.369</td> <td>   -3.036</td> <td>    1.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>    <td>    3.7859</td> <td>    4.550</td> <td>    0.832</td> <td> 0.405</td> <td>   -5.133</td> <td>   12.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>    <td>   -5.1564</td> <td>    2.909</td> <td>   -1.772</td> <td> 0.076</td> <td>  -10.858</td> <td>    0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>    <td>   -4.5580</td> <td>    2.959</td> <td>   -1.540</td> <td> 0.123</td> <td>  -10.358</td> <td>    1.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>    <td>    2.5287</td> <td>    1.742</td> <td>    1.451</td> <td> 0.147</td> <td>   -0.886</td> <td>    5.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>    <td>  -20.2889</td> <td>    8.096</td> <td>   -2.506</td> <td> 0.012</td> <td>  -36.156</td> <td>   -4.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>    <td>   30.1641</td> <td>    6.059</td> <td>    4.978</td> <td> 0.000</td> <td>   18.288</td> <td>   42.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>    <td>   14.8628</td> <td>    3.600</td> <td>    4.128</td> <td> 0.000</td> <td>    7.807</td> <td>   21.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>    <td>   -2.4173</td> <td>    6.316</td> <td>   -0.383</td> <td> 0.702</td> <td>  -14.797</td> <td>    9.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>    <td>    1.3286</td> <td>    1.189</td> <td>    1.117</td> <td> 0.264</td> <td>   -1.002</td> <td>    3.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>    <td>   -1.9366</td> <td>    2.848</td> <td>   -0.680</td> <td> 0.496</td> <td>   -7.518</td> <td>    3.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>    <td>    1.9142</td> <td>    1.316</td> <td>    1.454</td> <td> 0.146</td> <td>   -0.665</td> <td>    4.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>    <td>  -20.3493</td> <td>    5.950</td> <td>   -3.420</td> <td> 0.001</td> <td>  -32.011</td> <td>   -8.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>    <td>   -7.2963</td> <td>    4.335</td> <td>   -1.683</td> <td> 0.092</td> <td>  -15.792</td> <td>    1.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th> <td>   26.2084</td> <td>   14.624</td> <td>    1.792</td> <td> 0.073</td> <td>   -2.454</td> <td>   54.871</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.33 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               199364\n",
       "Model:                          Logit   Df Residuals:                   199334\n",
       "Method:                           MLE   Df Model:                           29\n",
       "Date:                Thu, 29 Apr 2021   Pseudo R-squ.:                  0.6921\n",
       "Time:                        01:15:47   Log-Likelihood:                -781.75\n",
       "converged:                       True   LL-Null:                       -2538.7\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Time          -0.6590      0.467     -1.411      0.158      -1.574       0.256\n",
       "V1             2.2145      2.727      0.812      0.417      -3.130       7.559\n",
       "V2            17.3211      5.955      2.909      0.004       5.649      28.993\n",
       "V3             1.1953      2.981      0.401      0.688      -4.648       7.038\n",
       "V4            17.9131      2.602      6.884      0.000      12.813      23.013\n",
       "V5            21.5616      5.615      3.840      0.000      10.557      32.567\n",
       "V6            -4.5520      4.258     -1.069      0.285     -12.897       3.793\n",
       "V7             1.1457      9.899      0.116      0.908     -18.257      20.548\n",
       "V8            -7.6560      5.709     -1.341      0.180     -18.845       3.533\n",
       "V9             2.3513      3.767      0.624      0.533      -5.032       9.735\n",
       "V10          -43.4730      8.089     -5.375      0.000     -59.326     -27.620\n",
       "V11            1.5285      1.615      0.946      0.344      -1.637       4.695\n",
       "V12            6.4494      2.839      2.271      0.023       0.884      12.015\n",
       "V13           -4.0916      1.071     -3.820      0.000      -6.191      -1.992\n",
       "V14          -17.6107      2.463     -7.151      0.000     -22.437     -12.784\n",
       "V15           -0.9541      1.062     -0.898      0.369      -3.036       1.128\n",
       "V16            3.7859      4.550      0.832      0.405      -5.133      12.704\n",
       "V17           -5.1564      2.909     -1.772      0.076     -10.858       0.546\n",
       "V18           -4.5580      2.959     -1.540      0.123     -10.358       1.242\n",
       "V19            2.5287      1.742      1.451      0.147      -0.886       5.944\n",
       "V20          -20.2889      8.096     -2.506      0.012     -36.156      -4.422\n",
       "V21           30.1641      6.059      4.978      0.000      18.288      42.040\n",
       "V22           14.8628      3.600      4.128      0.000       7.807      21.919\n",
       "V23           -2.4173      6.316     -0.383      0.702     -14.797       9.962\n",
       "V24            1.3286      1.189      1.117      0.264      -1.002       3.660\n",
       "V25           -1.9366      2.848     -0.680      0.496      -7.518       3.645\n",
       "V26            1.9142      1.316      1.454      0.146      -0.665       4.494\n",
       "V27          -20.3493      5.950     -3.420      0.001     -32.011      -8.687\n",
       "V28           -7.2963      4.335     -1.683      0.092     -15.792       1.200\n",
       "Amount        26.2084     14.624      1.792      0.073      -2.454      54.871\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.33 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logimodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "pretty-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train.reset_index(inplace=True,drop=True)\n",
    "scaled_test.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "general-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "shaped-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "overdispersion = (logimodel.llr*2/logimodel.df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dependent-rotation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03525582113973796"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overdispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "regulation-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'penalty':['l2','l1'],'solver':['lbfgs','liblinear','newton-cg','sag'],'C':[1,5,10]}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "lr = LogisticRegression(max_iter=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "documentary-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=lr,cv=10,param_grid = grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "classified-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.99903694 0.99903694 0.99903694 0.99903694        nan 0.99913726\n",
      "        nan        nan 0.99914729 0.99914227 0.99914729 0.99914729\n",
      "        nan 0.99917237        nan        nan 0.99915732 0.99916234\n",
      " 0.99915732 0.99915732        nan 0.99917237        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LogisticRegression(max_iter=500, n_jobs=-1),\n",
       "             param_grid={'C': [1, 5, 10], 'penalty': ['l2', 'l1'],\n",
       "                         'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag']})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "proof-veteran",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "arranged-fifteen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_grid = clf.predict(scaled_train)\n",
    "pred_test_grid = clf.predict(scaled_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.round(accuracy_score(y_train, pred_train_grid),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "decent-mason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(accuracy_score(y_test, pred_test_grid),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "chief-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "precise-scratch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    199019\n",
      "           1       0.89      0.62      0.73       345\n",
      "\n",
      "    accuracy                           1.00    199364\n",
      "   macro avg       0.95      0.81      0.86    199364\n",
      "weighted avg       1.00      1.00      1.00    199364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, pred_train_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "hollow-advancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.88      0.60      0.71       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.94      0.80      0.86     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "irish-aircraft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[198993,     26],\n",
       "       [   132,    213]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,pred_train_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "impressed-civilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85284,    12],\n",
       "       [   59,    88]], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "interior-plenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729068305828237"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_train,pred_train_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sunrise-testament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7121496106694271"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test,pred_test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "upper-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_df=pd.DataFrame()\n",
    "nir_df['predicted']=pred_train_grid\n",
    "nir_df['observed']=y_train.values\n",
    "nir_df['no_model_class']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "approved-respect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>observed</th>\n",
       "      <th>no_model_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199359</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199362</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199363</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199364 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted  observed  no_model_class\n",
       "0               0         0               0\n",
       "1               0         0               0\n",
       "2               0         0               0\n",
       "3               0         0               0\n",
       "4               0         0               0\n",
       "...           ...       ...             ...\n",
       "199359          0         0               0\n",
       "199360          0         0               0\n",
       "199361          0         0               0\n",
       "199362          0         0               0\n",
       "199363          0         0               0\n",
       "\n",
       "[199364 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "beginning-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982694970004614"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(nir_df['observed'],nir_df['no_model_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "specified-wilson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "breeding-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "turkish-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr,tpr,lw=3)\n",
    "plt.plot([0,1],[0,1],color='red',lw=3)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "horizontal-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(scaled_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "similar-catholic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.17238792e-05, 1.17238792e-05, 4.68955168e-05,\n",
       "       4.68955168e-05, 5.86193960e-05, 5.86193960e-05, 8.20671544e-05,\n",
       "       8.20671544e-05, 9.37910336e-05, 9.37910336e-05, 1.05514913e-04,\n",
       "       1.05514913e-04, 1.17238792e-04, 1.17238792e-04, 1.28962671e-04,\n",
       "       1.28962671e-04, 1.40686550e-04, 1.40686550e-04, 1.52410430e-04,\n",
       "       1.52410430e-04, 1.64134309e-04, 1.64134309e-04, 1.75858188e-04,\n",
       "       1.75858188e-04, 2.11029826e-04, 2.11029826e-04, 2.22753705e-04,\n",
       "       2.22753705e-04, 2.34477584e-04, 2.34477584e-04, 2.46201463e-04,\n",
       "       2.46201463e-04, 2.93096980e-04, 2.93096980e-04, 3.28268618e-04,\n",
       "       3.28268618e-04, 4.45507409e-04, 4.45507409e-04, 4.80679047e-04,\n",
       "       4.80679047e-04, 5.74470081e-04, 5.74470081e-04, 5.86193960e-04,\n",
       "       5.86193960e-04, 7.73776027e-04, 7.73776027e-04, 8.79290940e-04,\n",
       "       8.79290940e-04, 1.10204464e-03, 1.10204464e-03, 4.79506659e-03,\n",
       "       4.79506659e-03, 5.42815607e-03, 5.42815607e-03, 5.53367098e-03,\n",
       "       5.53367098e-03, 9.85978240e-03, 9.85978240e-03, 1.43617520e-02,\n",
       "       1.43617520e-02, 2.03878259e-02, 2.04112737e-02, 2.13491840e-02,\n",
       "       2.13491840e-02, 2.47139373e-02, 2.47373851e-02, 2.49249672e-02,\n",
       "       2.49484149e-02, 2.55228850e-02, 2.55228850e-02, 3.17130932e-02,\n",
       "       3.17130932e-02, 4.00722191e-02, 4.00722191e-02, 5.45981054e-02,\n",
       "       5.45981054e-02, 5.54656725e-02, 5.54891202e-02, 6.21717314e-02,\n",
       "       6.21951791e-02, 6.58530295e-02, 6.58530295e-02, 6.97453573e-02,\n",
       "       6.97805290e-02, 7.46693866e-02, 7.47045582e-02, 7.79520728e-02,\n",
       "       7.79872444e-02, 8.28526543e-02, 8.28761020e-02, 8.79408179e-02,\n",
       "       8.79642656e-02, 9.61709811e-02, 9.61944288e-02, 1.10603076e-01,\n",
       "       1.10638248e-01, 1.15925718e-01, 1.15925718e-01, 1.24812418e-01,\n",
       "       1.24835866e-01, 1.33699118e-01, 1.33722566e-01, 1.37040424e-01,\n",
       "       1.37087319e-01, 1.50604952e-01, 1.50628400e-01, 1.55329675e-01,\n",
       "       1.55353123e-01, 1.56185519e-01, 1.56220690e-01, 1.56654474e-01,\n",
       "       1.56654474e-01, 1.59175108e-01, 1.59198556e-01, 1.64216376e-01,\n",
       "       1.64239824e-01, 1.66256331e-01, 1.66303226e-01, 1.66537704e-01,\n",
       "       1.66631495e-01, 1.73818233e-01, 1.73841681e-01, 1.74216845e-01,\n",
       "       1.74240293e-01, 1.86222097e-01, 1.86245545e-01, 2.09505721e-01,\n",
       "       2.09505721e-01, 2.10713281e-01, 2.10736729e-01, 2.24055055e-01,\n",
       "       2.24078503e-01, 2.41781561e-01, 2.41805008e-01, 2.42731195e-01,\n",
       "       2.42754643e-01, 2.43669105e-01, 2.43692553e-01, 2.44431157e-01,\n",
       "       2.44454605e-01, 2.44466329e-01, 2.44466329e-01, 2.58429469e-01,\n",
       "       2.58452917e-01, 2.66518946e-01, 2.66542394e-01, 2.67703058e-01,\n",
       "       2.67726505e-01, 2.78993153e-01, 2.79016601e-01, 2.80130370e-01,\n",
       "       2.80153817e-01, 2.81220690e-01, 2.81244138e-01, 2.86132996e-01,\n",
       "       2.86156443e-01, 2.86977115e-01, 2.87000563e-01, 2.88372257e-01,\n",
       "       2.88395704e-01, 2.91350122e-01, 2.91350122e-01, 2.94210748e-01,\n",
       "       2.94234196e-01, 2.99076158e-01, 2.99111330e-01, 3.01069218e-01,\n",
       "       3.01092666e-01, 3.04398799e-01, 3.04433971e-01, 3.04527762e-01,\n",
       "       3.04551210e-01, 3.19592947e-01, 3.19616395e-01, 3.23473551e-01,\n",
       "       3.23473551e-01, 3.29417558e-01, 3.29441005e-01, 3.42829675e-01,\n",
       "       3.42853123e-01, 3.59653442e-01, 3.59676890e-01, 3.71353874e-01,\n",
       "       3.71377321e-01, 3.74448978e-01, 3.74472425e-01, 3.86290096e-01,\n",
       "       3.86313543e-01, 3.86383887e-01, 3.86407334e-01, 3.95856781e-01,\n",
       "       3.95880229e-01, 4.00089101e-01, 4.00112549e-01, 4.01355280e-01,\n",
       "       4.01378728e-01, 4.05470362e-01, 4.05493810e-01, 4.05892422e-01,\n",
       "       4.05915869e-01, 4.12856406e-01, 4.12879854e-01, 4.21274151e-01,\n",
       "       4.21297599e-01, 4.24263740e-01, 4.24287188e-01, 4.33431814e-01,\n",
       "       4.33455262e-01, 4.51216939e-01, 4.51240386e-01, 4.51345901e-01,\n",
       "       4.51345901e-01, 4.52612080e-01, 4.52635528e-01, 4.54769274e-01,\n",
       "       4.54792722e-01, 4.65203527e-01, 4.65238698e-01, 4.66973832e-01,\n",
       "       4.66997280e-01, 4.68451041e-01, 4.68451041e-01, 5.16753423e-01,\n",
       "       5.16776871e-01, 5.17198931e-01, 5.17198931e-01, 5.18957513e-01,\n",
       "       5.18980960e-01, 5.19227162e-01, 5.19250610e-01, 5.20974020e-01,\n",
       "       5.20997468e-01, 5.31267586e-01, 5.31291034e-01, 5.34644063e-01,\n",
       "       5.34667511e-01, 5.35406115e-01, 5.35429563e-01, 5.41397017e-01,\n",
       "       5.41397017e-01, 5.43179047e-01, 5.43202495e-01, 5.69311574e-01,\n",
       "       5.69335022e-01, 5.81985087e-01, 5.82008535e-01, 5.82125774e-01,\n",
       "       5.82149222e-01, 5.95326862e-01, 5.95350310e-01, 6.06992122e-01,\n",
       "       6.07015569e-01, 6.13534046e-01, 6.13557494e-01, 6.14284374e-01,\n",
       "       6.14307822e-01, 6.15480210e-01, 6.15480210e-01, 6.29584037e-01,\n",
       "       6.29607485e-01, 6.67663196e-01, 6.67686644e-01, 6.74650628e-01,\n",
       "       6.74674076e-01, 6.96937723e-01, 6.96961171e-01, 7.39483680e-01,\n",
       "       7.39507128e-01, 7.39682986e-01, 7.39706434e-01, 7.55510223e-01,\n",
       "       7.55533671e-01, 7.84890264e-01, 7.84913712e-01, 7.90212906e-01,\n",
       "       7.90236353e-01, 7.90717032e-01, 7.90740480e-01, 7.93683174e-01,\n",
       "       7.93706622e-01, 8.13297224e-01, 8.13320672e-01, 8.18678484e-01,\n",
       "       8.18701932e-01, 8.52161883e-01, 8.52197055e-01, 8.52290846e-01,\n",
       "       8.52314294e-01, 8.66957419e-01, 8.66980867e-01, 8.81436410e-01,\n",
       "       8.81459857e-01, 8.83429469e-01, 8.83464641e-01, 8.84355656e-01,\n",
       "       8.84390827e-01, 8.86747327e-01, 8.86770775e-01, 8.95211968e-01,\n",
       "       8.95235415e-01, 8.97287094e-01, 8.97310542e-01, 9.06795160e-01,\n",
       "       9.06818608e-01, 9.13946727e-01, 9.13970174e-01, 9.14779122e-01,\n",
       "       9.14802570e-01, 9.15646689e-01, 9.15670137e-01, 9.16807353e-01,\n",
       "       9.16830801e-01, 9.83387263e-01, 9.83410711e-01, 9.87009942e-01,\n",
       "       9.87033390e-01, 9.91558807e-01, 9.91593979e-01, 9.91617426e-01,\n",
       "       9.93856687e-01, 9.93880135e-01, 9.95146314e-01, 9.95169762e-01,\n",
       "       9.97244888e-01, 9.97268336e-01, 9.99155881e-01, 9.99179328e-01,\n",
       "       1.00000000e+00])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "convenient-century",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00680272, 0.03401361, 0.04761905, 0.19047619,\n",
       "       0.19047619, 0.20408163, 0.20408163, 0.21768707, 0.21768707,\n",
       "       0.29251701, 0.29251701, 0.31292517, 0.31292517, 0.33333333,\n",
       "       0.33333333, 0.34013605, 0.34013605, 0.42176871, 0.42176871,\n",
       "       0.56462585, 0.56462585, 0.63265306, 0.63265306, 0.63945578,\n",
       "       0.63945578, 0.6462585 , 0.6462585 , 0.65986395, 0.65986395,\n",
       "       0.68707483, 0.68707483, 0.71428571, 0.71428571, 0.73469388,\n",
       "       0.73469388, 0.7755102 , 0.7755102 , 0.78911565, 0.78911565,\n",
       "       0.80272109, 0.80272109, 0.80952381, 0.80952381, 0.81632653,\n",
       "       0.81632653, 0.82312925, 0.82312925, 0.82993197, 0.82993197,\n",
       "       0.83673469, 0.83673469, 0.84353741, 0.84353741, 0.85034014,\n",
       "       0.85034014, 0.85714286, 0.85714286, 0.86394558, 0.86394558,\n",
       "       0.8707483 , 0.8707483 , 0.87755102, 0.87755102, 0.88435374,\n",
       "       0.88435374, 0.88435374, 0.88435374, 0.89115646, 0.89115646,\n",
       "       0.89115646, 0.89115646, 0.89115646, 0.89115646, 0.89795918,\n",
       "       0.89795918, 0.9047619 , 0.9047619 , 0.91156463, 0.91156463,\n",
       "       0.91836735, 0.91836735, 0.91836735, 0.91836735, 0.91836735,\n",
       "       0.91836735, 0.92517007, 0.92517007, 0.92517007, 0.92517007,\n",
       "       0.92517007, 0.92517007, 0.92517007, 0.92517007, 0.92517007,\n",
       "       0.92517007, 0.92517007, 0.92517007, 0.92517007, 0.92517007,\n",
       "       0.92517007, 0.92517007, 0.93197279, 0.93197279, 0.93197279,\n",
       "       0.93197279, 0.93197279, 0.93197279, 0.93197279, 0.93197279,\n",
       "       0.93197279, 0.93197279, 0.93197279, 0.93197279, 0.93197279,\n",
       "       0.93197279, 0.93877551, 0.93877551, 0.93877551, 0.93877551,\n",
       "       0.93877551, 0.93877551, 0.93877551, 0.93877551, 0.93877551,\n",
       "       0.93877551, 0.93877551, 0.93877551, 0.93877551, 0.93877551,\n",
       "       0.93877551, 0.93877551, 0.94557823, 0.94557823, 0.94557823,\n",
       "       0.94557823, 0.94557823, 0.94557823, 0.94557823, 0.94557823,\n",
       "       0.94557823, 0.94557823, 0.94557823, 0.94557823, 0.94557823,\n",
       "       0.94557823, 0.95238095, 0.95238095, 0.95238095, 0.95238095,\n",
       "       0.95238095, 0.95238095, 0.95238095, 0.95238095, 0.95238095,\n",
       "       0.95238095, 0.95238095, 0.95238095, 0.95238095, 0.95238095,\n",
       "       0.95238095, 0.95238095, 0.95238095, 0.95238095, 0.95238095,\n",
       "       0.95238095, 0.95918367, 0.95918367, 0.95918367, 0.95918367,\n",
       "       0.95918367, 0.95918367, 0.95918367, 0.95918367, 0.95918367,\n",
       "       0.95918367, 0.95918367, 0.95918367, 0.95918367, 0.95918367,\n",
       "       0.96598639, 0.96598639, 0.96598639, 0.96598639, 0.96598639,\n",
       "       0.96598639, 0.96598639, 0.96598639, 0.96598639, 0.96598639,\n",
       "       0.96598639, 0.96598639, 0.96598639, 0.96598639, 0.96598639,\n",
       "       0.96598639, 0.96598639, 0.96598639, 0.96598639, 0.96598639,\n",
       "       0.96598639, 0.96598639, 0.96598639, 0.96598639, 0.96598639,\n",
       "       0.96598639, 0.96598639, 0.96598639, 0.96598639, 0.96598639,\n",
       "       0.96598639, 0.96598639, 0.96598639, 0.96598639, 0.96598639,\n",
       "       0.96598639, 0.97278912, 0.97278912, 0.97278912, 0.97278912,\n",
       "       0.97278912, 0.97278912, 0.97278912, 0.97278912, 0.97278912,\n",
       "       0.97278912, 0.97959184, 0.97959184, 0.97959184, 0.97959184,\n",
       "       0.98639456, 0.98639456, 0.98639456, 0.98639456, 0.98639456,\n",
       "       0.98639456, 0.98639456, 0.98639456, 0.98639456, 0.98639456,\n",
       "       0.98639456, 0.98639456, 0.98639456, 0.98639456, 0.99319728,\n",
       "       0.99319728, 0.99319728, 0.99319728, 0.99319728, 0.99319728,\n",
       "       0.99319728, 0.99319728, 0.99319728, 0.99319728, 0.99319728,\n",
       "       0.99319728, 0.99319728, 0.99319728, 0.99319728, 0.99319728,\n",
       "       0.99319728, 0.99319728, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "disturbed-academy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00057107, 0.00035313, 0.00064535, ..., 0.00053997, 0.0003015 ,\n",
       "       0.00128621])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "finite-submission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9712114164742993"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "declared-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn .metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "minus-motion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9712114164742993"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "published-snapshot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728164</td>\n",
       "      <td>0.943715</td>\n",
       "      <td>0.754102</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.222346</td>\n",
       "      <td>0.569302</td>\n",
       "      <td>0.493624</td>\n",
       "      <td>0.510392</td>\n",
       "      <td>0.784052</td>\n",
       "      <td>0.456764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>0.558141</td>\n",
       "      <td>0.489790</td>\n",
       "      <td>0.661548</td>\n",
       "      <td>0.405524</td>\n",
       "      <td>0.589779</td>\n",
       "      <td>0.423942</td>\n",
       "      <td>0.653094</td>\n",
       "      <td>0.345811</td>\n",
       "      <td>0.002035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.909967</td>\n",
       "      <td>0.943180</td>\n",
       "      <td>0.752640</td>\n",
       "      <td>0.785443</td>\n",
       "      <td>0.220108</td>\n",
       "      <td>0.568122</td>\n",
       "      <td>0.480092</td>\n",
       "      <td>0.508690</td>\n",
       "      <td>0.784623</td>\n",
       "      <td>0.452374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373414</td>\n",
       "      <td>0.557102</td>\n",
       "      <td>0.471162</td>\n",
       "      <td>0.664983</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>0.555183</td>\n",
       "      <td>0.444960</td>\n",
       "      <td>0.652182</td>\n",
       "      <td>0.346231</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.882396</td>\n",
       "      <td>0.917525</td>\n",
       "      <td>0.713541</td>\n",
       "      <td>0.789698</td>\n",
       "      <td>0.279482</td>\n",
       "      <td>0.583853</td>\n",
       "      <td>0.458116</td>\n",
       "      <td>0.484161</td>\n",
       "      <td>0.788924</td>\n",
       "      <td>0.493915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381601</td>\n",
       "      <td>0.572786</td>\n",
       "      <td>0.562955</td>\n",
       "      <td>0.679766</td>\n",
       "      <td>0.505973</td>\n",
       "      <td>0.476756</td>\n",
       "      <td>0.248970</td>\n",
       "      <td>0.658215</td>\n",
       "      <td>0.353856</td>\n",
       "      <td>0.004884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294730</td>\n",
       "      <td>0.900033</td>\n",
       "      <td>0.751797</td>\n",
       "      <td>0.815605</td>\n",
       "      <td>0.296788</td>\n",
       "      <td>0.528430</td>\n",
       "      <td>0.539393</td>\n",
       "      <td>0.502993</td>\n",
       "      <td>0.782775</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359282</td>\n",
       "      <td>0.574544</td>\n",
       "      <td>0.542324</td>\n",
       "      <td>0.667663</td>\n",
       "      <td>0.374244</td>\n",
       "      <td>0.578197</td>\n",
       "      <td>0.392906</td>\n",
       "      <td>0.635278</td>\n",
       "      <td>0.321877</td>\n",
       "      <td>0.015669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.115162</td>\n",
       "      <td>0.977426</td>\n",
       "      <td>0.735312</td>\n",
       "      <td>0.799246</td>\n",
       "      <td>0.240071</td>\n",
       "      <td>0.538221</td>\n",
       "      <td>0.484471</td>\n",
       "      <td>0.488015</td>\n",
       "      <td>0.784702</td>\n",
       "      <td>0.497134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372932</td>\n",
       "      <td>0.559232</td>\n",
       "      <td>0.506545</td>\n",
       "      <td>0.666429</td>\n",
       "      <td>0.461001</td>\n",
       "      <td>0.599186</td>\n",
       "      <td>0.377757</td>\n",
       "      <td>0.649889</td>\n",
       "      <td>0.341213</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85438</th>\n",
       "      <td>0.792595</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.750058</td>\n",
       "      <td>0.797817</td>\n",
       "      <td>0.231488</td>\n",
       "      <td>0.561552</td>\n",
       "      <td>0.486834</td>\n",
       "      <td>0.504764</td>\n",
       "      <td>0.786457</td>\n",
       "      <td>0.449722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375211</td>\n",
       "      <td>0.559835</td>\n",
       "      <td>0.491768</td>\n",
       "      <td>0.660495</td>\n",
       "      <td>0.337462</td>\n",
       "      <td>0.619818</td>\n",
       "      <td>0.483956</td>\n",
       "      <td>0.645622</td>\n",
       "      <td>0.337878</td>\n",
       "      <td>0.001524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85439</th>\n",
       "      <td>0.924696</td>\n",
       "      <td>0.855479</td>\n",
       "      <td>0.698206</td>\n",
       "      <td>0.747602</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.564619</td>\n",
       "      <td>0.484974</td>\n",
       "      <td>0.531345</td>\n",
       "      <td>0.772642</td>\n",
       "      <td>0.492567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328534</td>\n",
       "      <td>0.539119</td>\n",
       "      <td>0.529387</td>\n",
       "      <td>0.711520</td>\n",
       "      <td>0.506123</td>\n",
       "      <td>0.603953</td>\n",
       "      <td>0.427186</td>\n",
       "      <td>0.678019</td>\n",
       "      <td>0.295482</td>\n",
       "      <td>0.027118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85440</th>\n",
       "      <td>0.323574</td>\n",
       "      <td>0.925623</td>\n",
       "      <td>0.757874</td>\n",
       "      <td>0.792311</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.513349</td>\n",
       "      <td>0.486033</td>\n",
       "      <td>0.764256</td>\n",
       "      <td>0.431163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385402</td>\n",
       "      <td>0.541641</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>0.418568</td>\n",
       "      <td>0.575289</td>\n",
       "      <td>0.380610</td>\n",
       "      <td>0.656719</td>\n",
       "      <td>0.340088</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85441</th>\n",
       "      <td>0.225097</td>\n",
       "      <td>0.937446</td>\n",
       "      <td>0.755915</td>\n",
       "      <td>0.814359</td>\n",
       "      <td>0.256407</td>\n",
       "      <td>0.548283</td>\n",
       "      <td>0.467345</td>\n",
       "      <td>0.504560</td>\n",
       "      <td>0.784885</td>\n",
       "      <td>0.459215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377653</td>\n",
       "      <td>0.557147</td>\n",
       "      <td>0.477949</td>\n",
       "      <td>0.666659</td>\n",
       "      <td>0.534801</td>\n",
       "      <td>0.566261</td>\n",
       "      <td>0.430347</td>\n",
       "      <td>0.659914</td>\n",
       "      <td>0.346023</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85442</th>\n",
       "      <td>0.872760</td>\n",
       "      <td>0.945145</td>\n",
       "      <td>0.763429</td>\n",
       "      <td>0.785602</td>\n",
       "      <td>0.439085</td>\n",
       "      <td>0.565527</td>\n",
       "      <td>0.519686</td>\n",
       "      <td>0.500607</td>\n",
       "      <td>0.790719</td>\n",
       "      <td>0.387818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379791</td>\n",
       "      <td>0.567627</td>\n",
       "      <td>0.569911</td>\n",
       "      <td>0.661835</td>\n",
       "      <td>0.427066</td>\n",
       "      <td>0.558868</td>\n",
       "      <td>0.527104</td>\n",
       "      <td>0.660344</td>\n",
       "      <td>0.347142</td>\n",
       "      <td>0.000539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85443 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0.728164  0.943715  0.754102  0.781000  0.222346  0.569302  0.493624   \n",
       "1      0.909967  0.943180  0.752640  0.785443  0.220108  0.568122  0.480092   \n",
       "2      0.882396  0.917525  0.713541  0.789698  0.279482  0.583853  0.458116   \n",
       "3      0.294730  0.900033  0.751797  0.815605  0.296788  0.528430  0.539393   \n",
       "4      0.115162  0.977426  0.735312  0.799246  0.240071  0.538221  0.484471   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "85438  0.792595  0.941571  0.750058  0.797817  0.231488  0.561552  0.486834   \n",
       "85439  0.924696  0.855479  0.698206  0.747602  0.286822  0.564619  0.484974   \n",
       "85440  0.323574  0.925623  0.757874  0.792311  0.312200  0.547832  0.513349   \n",
       "85441  0.225097  0.937446  0.755915  0.814359  0.256407  0.548283  0.467345   \n",
       "85442  0.872760  0.945145  0.763429  0.785602  0.439085  0.565527  0.519686   \n",
       "\n",
       "             V7        V8        V9  ...       V20       V21       V22  \\\n",
       "0      0.510392  0.784052  0.456764  ...  0.377900  0.558141  0.489790   \n",
       "1      0.508690  0.784623  0.452374  ...  0.373414  0.557102  0.471162   \n",
       "2      0.484161  0.788924  0.493915  ...  0.381601  0.572786  0.562955   \n",
       "3      0.502993  0.782775  0.453800  ...  0.359282  0.574544  0.542324   \n",
       "4      0.488015  0.784702  0.497134  ...  0.372932  0.559232  0.506545   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "85438  0.504764  0.786457  0.449722  ...  0.375211  0.559835  0.491768   \n",
       "85439  0.531345  0.772642  0.492567  ...  0.328534  0.539119  0.529387   \n",
       "85440  0.486033  0.764256  0.431163  ...  0.385402  0.541641  0.543479   \n",
       "85441  0.504560  0.784885  0.459215  ...  0.377653  0.557147  0.477949   \n",
       "85442  0.500607  0.790719  0.387818  ...  0.379791  0.567627  0.569911   \n",
       "\n",
       "            V23       V24       V25       V26       V27       V28    Amount  \n",
       "0      0.661548  0.405524  0.589779  0.423942  0.653094  0.345811  0.002035  \n",
       "1      0.664983  0.483923  0.555183  0.444960  0.652182  0.346231  0.000101  \n",
       "2      0.679766  0.505973  0.476756  0.248970  0.658215  0.353856  0.004884  \n",
       "3      0.667663  0.374244  0.578197  0.392906  0.635278  0.321877  0.015669  \n",
       "4      0.666429  0.461001  0.599186  0.377757  0.649889  0.341213  0.000254  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "85438  0.660495  0.337462  0.619818  0.483956  0.645622  0.337878  0.001524  \n",
       "85439  0.711520  0.506123  0.603953  0.427186  0.678019  0.295482  0.027118  \n",
       "85440  0.667037  0.418568  0.575289  0.380610  0.656719  0.340088  0.002900  \n",
       "85441  0.666659  0.534801  0.566261  0.430347  0.659914  0.346023  0.000364  \n",
       "85442  0.661835  0.427066  0.558868  0.527104  0.660344  0.347142  0.000539  \n",
       "\n",
       "[85443 rows x 30 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ethical-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba1 = clf.predict_proba(scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "supposed-coordinator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00057107, 0.00035313, 0.00064535, ..., 0.00053997, 0.0003015 ,\n",
       "       0.00128621])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "promising-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba1=pd.DataFrame(data=y_pred_proba1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cubic-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba2 = clf.predict_proba(scaled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-walter",
   "metadata": {},
   "source": [
    "# since the data is imbalace we use SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_res,y_train_res=sm.fit_resample(scaled_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "exciting-resident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199019\n",
       "1    199019\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "green-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_res,y_test_res=sm.fit_resample(scaled_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "contemporary-alaska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    85296\n",
       "1    85296\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "therapeutic-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(penalty='l1',solver='liblinear',C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "handy-cemetery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(x_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "handed-quantum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_grid1 = clf1.predict(x_train_res)\n",
    "pred_test_grid1 = clf1.predict(x_test_res)\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.round(accuracy_score(y_train_res, pred_train_grid1),2)\n",
    "np.round(accuracy_score(y_test_res, pred_test_grid1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "helpful-employer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(accuracy_score(y_train_res, pred_train_grid1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "acting-irrigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(accuracy_score(y_test_res, pred_test_grid1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "small-borough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95    199019\n",
      "           1       0.97      0.92      0.95    199019\n",
      "\n",
      "    accuracy                           0.95    398038\n",
      "   macro avg       0.95      0.95      0.95    398038\n",
      "weighted avg       0.95      0.95      0.95    398038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_res, pred_train_grid1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "secondary-services",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95     85296\n",
      "           1       0.98      0.92      0.95     85296\n",
      "\n",
      "    accuracy                           0.95    170592\n",
      "   macro avg       0.95      0.95      0.95    170592\n",
      "weighted avg       0.95      0.95      0.95    170592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_res, pred_test_grid1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "frank-problem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[194254,   4765],\n",
       "       [ 15113, 183906]], dtype=int64)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train_res,pred_train_grid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "valuable-complexity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001200890367251"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_train_res,pred_train_grid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cubic-twins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83318,  1978],\n",
       "       [ 6878, 78418]], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_res,pred_test_grid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "premium-classification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8961733258300506"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test_res,pred_test_grid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "instrumental-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba3 = clf1.predict_proba(x_test_res)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test_res, y_pred_proba3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "stuffed-gabriel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr,tpr,lw=3)\n",
    "plt.plot([0,1],[0,1],color='red',lw=3)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ruled-marshall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877547702201487"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-luxury",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
